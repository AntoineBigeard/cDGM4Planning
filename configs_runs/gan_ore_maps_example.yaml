name_experiment: ore_maps_ddpm_100           # name under which the experiment will be saved
seed: 1234                                   # torch seed for experiments reproductibility
mode: fit                                    # mode for the experiment: fit, test or predict

datamodule:
  path_surfaces_h5py: train_data_x.hdf5      # path to data containing the subsurfaces
  path_observations_h5py: test_data_y.hdf5   # path to data containing the associated conditions
  batch_size: 4                              # batch size
  shuffle_data: True                         # True to shuffle data (before the splitting fit/test)
  num_workers: 4                             # number of CPU workers
  pct_train: 0.9                             # percentage of data used for training
  pct_val: 0.08                              # percentage of data used for validation
  pct_test: 0.02                             # percentage of data used for testing
  two_dimensional: True                      # True if the data is two dimensional

lit_model_type: LitDCGAN2d
lit_model:
  generator: LargeGeneratorInject2d          # generator model (see DCGAN2d.py)
  discriminator: LargerDiscriminator2d       # discriminator model (see DCGAN2d.py)
  conf_generator:
    latent_dim: 128                          # dimension of the latent vector
    layers: [                                # definition of the generator layers, see blocks.py for  more details about the blocks
      {Cv2d: [128, 128, [4,4], [2,2], [1,1]], BN2d: [128], LR: null, UpSample: [null, 2, bilinear]},
      {Cv2d: [128, 256, [3,3], [1,1], [1,1]], BN2d: [256], LR: null, D: [0.25,], UpSample: [null, 2, bilinear]},
      {Cv2d: [256, 512, [4,4], [2,2], [1,1]], BN2d: [512], LR: null},
      {Cv2d: [512, 512, [3,3], [1,1], [1,1]], LR: null},
      {Cv2d: [512, 256, [3,3], [1,1], [1,1]], LR: null},
      {Cv2d: [256, 256, [3,3], [1,1], [1,1]], LR: null},
      {Cv2d: [256, 128, [3,3], [1,1], [1,1]], LR: null},
      {Cv2d: [128, 1, [3,3], [1,1], [1,1]]},
    ]
    injections: [                            # choose where to inject the condition
                                             # [True to inject,
                                             # out channels of the encoding convolution (if 0 injects without encoding)
                                             # scaling factor (basic upsample/downsample is applied)]
      [True, 8, 1],
      [True, 8, 1],
      [True, 8, 2],
      [True, 8, 1],
      [True, 8, 1],
      [True, 8, 1],
      [True, 8, 1],
      [True, 8, 1],
    ]
  conf_discriminator:
    layers: [                                # definition of the discriminator layers, see blocks.py for more details about the blocks
      {Cv2d: [3, 128, 3, 2, 1]},
      {LR: [0.2], D: [0.4]},
      {Cv2d: [128, 256, 3, 2, 1]},
      {LR: [0.2], D: [0.4], LR: [0.2]},
      {Cv2d: [256, 256, 3, 1, 1]},
      {LR: [0.2], D: [0.4], LR: [0.2]},
      {Cv2d: [256, 128, 3, 1, 1]},
      {LR: [0.2], D: [0.4], LR: [0.2]},
      {Cv2d: [128, 128, 3, 1, 1]},
      {LR: [0.2], D: [0.4], LR: [0.2]},
      {Cv2d: [128, 64, 3, 1, 1]},
      {LR: [0.2], D: [0.4], LR: [0.2]},
      {Cv2d: [64, 32, 3, 1, 1]},
      {D: [0.4]},
      {F: null},
      {L: [2048, 1]},
    ]
    spectral_norm: [                         # True to use spectral normalisation for the discriminator layers
      False,
      False,
      False,
      False,
      False,
      False,
      False,
      False,
      False,
      False,
      False,
      False,
      False,
      False,
      False,
      False,
    ]
  g_lr: 0.0002                               # generator learning rate
  d_lr: 0.0002                               # discriminator learning rate
  b1: 0.5                                    # Adam optimizer parameter b1
  b2: 0.999                                  # Adam optimizer parameter b2
  validation_x_shape: [10, 1, 32, 32]        # shape of for the tests made in validation
  use_rd_y: True                             # True to generate new random conditions at each training step
  wasserstein_gp_loss: False                 # False for Cross-Entropy loss, "w" for Wasserstein loss, "wgp" for Wassertein Gradient Penalty loss
  n_sample_for_metric: 100                   # number of samples used for each data point when computing metrics
  latent_1d: True                            # True to use 1dimensional latent vector and a Dense encoding before shaping it in 2d

trainer:
  auto_lr_find: False                        # True to let PytorchLightning find the learning rate
  min_epochs: 1                              # minimum of epochs for training
  max_epochs: 50                             # maximum of epochs for training
  fast_dev_run: false                        # True to run in debug
  accelerator: gpu
  devices: [0]
  log_every_n_steps: 10                      # saving frequency for the logs

tensorboard_logs:
  save_dir: logs                             # directory in which to save experiments               

checkpoint_callback: {}                      # optional use of checkpoints, not used for GANs
  # monitor: val/loss                        # metric to monitor
  # mode: min                                # how to monitor the metric
  # save_last: True                          # True to always save the last checkpoint
  # filename: best-checkpoint                # name of the checkpoint saved using the monitor and mode

checkpoint_path: logs/checkpoint.ckpt        # path to the checkpoint to use as a start for the weights