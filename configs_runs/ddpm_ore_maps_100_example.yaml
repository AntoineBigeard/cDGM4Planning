name_experiment: ore_maps_ddpm_100           # name under which the experiment will be saved
seed: 1234                                   # torch seed for experiments reproductibility
mode: fit                                    # mode for the experiment: fit, test or predict

datamodule:
  path_surfaces_h5py: train_data_x.hdf5      # path to data containing the subsurfaces
  path_observations_h5py: test_data_y.hdf5   # path to data containing the associated conditions
  batch_size: 4                              # batch size
  shuffle_data: True                         # True to shuffle data (before the splitting fit/test)
  num_workers: 4                             # number of CPU workers
  pct_train: 0.9                             # percentage of data used for training
  pct_val: 0.08                              # percentage of data used for validation
  pct_test: 0.02                             # percentage of data used for testing
  two_dimensional: True                      # True if the data is two dimensional

lit_model_type: LitDDPM2d                    # lightning model to use: LitDDPM2d, LitDCGAN2d
lit_model:
  diffusion: Diffusion2d                     # diffusion model (always Diffusion2d for now)
  conf_diffusion:                       
    noise_steps: 100                         # number of denoising steps for the diffusion process
    beta_start: 0.0001                       # beta start for noise schedule
    beta_end: 0.02                           # beta end for noise schedule
    surf_size: 32                            # subsurface (data) resolution
  ddpm: UNet_conditional2d                   # model for denoising
  conf_ddpm:
    c_in: 3                                  # number of in channels for the model
    c_out: 1                                 # number of out channels for the model
    time_dim: 256                            # dimension of the time encoding
  ema: EMA2d
  conf_ema:
    beta: 0.995                              # beta for the EMA model associated to the denoising model
  lr: 0.0002                                 # learning rate
  b1: 0.5                                    # b1 for Adam optimizer
  b2: 0.999                                  # b2 for Adam optimizer
  latent_dim: 20                             # dimension of latent space (not used for DDPMS)
  validation_x_shape: [1, 1, 32, 32]         # shape of inference validation data between (first parameter corresponds to batch size)
  use_rd_y: True                             # True to generate random conditions at each training step, and thus avoid overfitting
  batch_size: 100                            # batch size, not taken into account, batch size from datamodule is the only one considered
  cfg_scale: 0                               # option for denoising model to combine conditional and unconditional predictions (slows the process)
  n_sample_for_metric: 100                   # number of samples to make for the same condition, in order to compute the metrics

trainer:
  auto_lr_find: False                        # True to let PytorchLightning find the learning rate
  min_epochs: 1                              # minimum of epochs for training
  max_epochs: 50                             # maximum of epochs for training
  fast_dev_run: false                        # True to run in debug
  accelerator: gpu
  devices: [0]
  log_every_n_steps: 10                      # saving frequency for the logs

tensorboard_logs:
  save_dir: logs                             # directory in which to save experiments               
#   name: test_res_inject

checkpoint_callback:                         # optional use of checkpoints
  monitor: val/loss                          # metric to monitor
  mode: min                                  # how to monitor the metric
  save_last: True                            # True to always save the last checkpoint
  filename: best-checkpoint                  # name of the checkpoint saved using the monitor and mode

checkpoint_path: logs/checkpoint.ckpt        # path to the checkpoint to use as a start for the weights